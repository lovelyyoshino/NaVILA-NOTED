# LLaVA/VILA æ¨¡å—è¯´æ˜æ–‡æ¡£

## ğŸ“– æ¨¡å—æ¦‚è¿°

`llava/` æ¨¡å—æ˜¯NaVILAé¡¹ç›®çš„æ ¸å¿ƒå¤šæ¨¡æ€å¤„ç†æ¨¡å—ï¼ŒåŸºäºLLaVA (Large Language and Vision Assistant) æ¶æ„ã€‚è¯¥æ¨¡å—æä¾›äº†å®Œæ•´çš„è§†è§‰-è¯­è¨€æ¨¡å‹å®ç°ï¼Œæ”¯æŒå›¾åƒç†è§£ã€è§†é¢‘ç†è§£å’Œè§†è§‰-è¯­è¨€å¯¼èˆªä»»åŠ¡ã€‚

**æ ¸å¿ƒç‰¹æ€§ï¼š**
- âœ¨ å¤šæ¨¡æ€èåˆï¼šæ”¯æŒå›¾åƒã€è§†é¢‘ã€æ–‡æœ¬çš„ç»Ÿä¸€å¤„ç†
- ğŸ”§ æ¨¡å—åŒ–è®¾è®¡ï¼šå¯è‡ªç”±ç»„åˆè¯­è¨€æ¨¡å‹ã€è§†è§‰ç¼–ç å™¨å’ŒæŠ•å½±å™¨
- ğŸš€ åˆ†å¸ƒå¼è®­ç»ƒï¼šæ”¯æŒå¤šèŠ‚ç‚¹ã€å¤šGPUçš„é«˜æ•ˆè®­ç»ƒ
- ğŸ“Š ä»»åŠ¡è¯„ä¼°ï¼šå†…ç½®å¤šç§benchmarkçš„è¯„ä¼°å·¥å…·
- ğŸ¯ å¯¼èˆªä»»åŠ¡ï¼šä¸“é—¨ä¼˜åŒ–çš„è§†è§‰-è¯­è¨€å¯¼èˆªèƒ½åŠ›

## ğŸ—ï¸ ç›®å½•ç»“æ„è¯¦è§£

### ğŸ“ æ ¹ç›®å½•æ–‡ä»¶

```
llava/
â”œâ”€â”€ __init__.py                 # æ¨¡å—åˆå§‹åŒ–æ–‡ä»¶
â”‚                                # å¯¼å‡ºæ ¸å¿ƒåŠŸèƒ½: load(), Image, Videoç­‰
â”‚
â”œâ”€â”€ constants.py                # å…¨å±€å¸¸é‡å®šä¹‰
â”‚                                # - ç‰¹æ®Štokenç´¢å¼• (IMAGE_TOKEN_INDEX, IGNORE_INDEX)
â”‚                                # - ç‰¹æ®Štokenå­—ç¬¦ä¸² (<image>, <video>ç­‰)
â”‚                                # - VILAå“¨å…µæ ‡è®°
â”‚
â”œâ”€â”€ conversation.py             # å¯¹è¯ç®¡ç†ç³»ç»Ÿ â­æ ¸å¿ƒ
â”‚                                # - SeparatorStyle: ä¸åŒå¯¹è¯æ ¼å¼çš„åˆ†éš”ç¬¦æ ·å¼
â”‚                                # - Conversation: å¯¹è¯çŠ¶æ€ç®¡ç†ç±»
â”‚                                # - é¢„å®šä¹‰æ¨¡æ¿: llama_3, mistral, vicunaç­‰
â”‚                                # - auto_set_conversation_mode(): è‡ªåŠ¨é€‰æ‹©æ¨¡æ¿
â”‚
â”œâ”€â”€ entry.py                    # æ¨¡å‹åŠ è½½å…¥å£ â­æ¨è
â”‚                                # - load(): ç®€åŒ–çš„æ¨¡å‹åŠ è½½æ¥å£
â”‚                                # - è‡ªåŠ¨é…ç½®GPUåˆ†é…å’Œå†…å­˜ç®¡ç†
â”‚
â”œâ”€â”€ media.py                    # åª’ä½“ç±»å‹å®šä¹‰
â”‚                                # - Media: åª’ä½“åŸºç±»
â”‚                                # - File: æ–‡ä»¶åª’ä½“ç±»
â”‚                                # - Image: å›¾åƒç±»
â”‚                                # - Video: è§†é¢‘ç±»
â”‚
â””â”€â”€ mm_utils.py                 # å¤šæ¨¡æ€å·¥å…·å‡½æ•° â­é‡è¦
                                 # - vlnce_frame_sampling(): VLN-CEä»»åŠ¡çš„å¸§é‡‡æ ·
                                 # - opencv_extract_frames(): è§†é¢‘å¸§æå–
                                 # - process_images(): å›¾åƒé¢„å¤„ç†
                                 # - tokenizer_image_token(): ç‰¹æ®Štokenå¤„ç†
                                 # - KeywordsStoppingCriteria: ç”Ÿæˆåœæ­¢æ¡ä»¶
```

### ğŸ“ cli/ - å‘½ä»¤è¡Œå·¥å…·

```
cli/
â”œâ”€â”€ run.py                      # SLURMä»»åŠ¡è¿è¡Œå·¥å…· â­é›†ç¾¤å¿…å¤‡
â”‚                                # åŠŸèƒ½:
â”‚                                # - ç®€åŒ–SLURMä»»åŠ¡æäº¤æµç¨‹
â”‚                                # - æ”¯æŒä»»åŠ¡è¶…æ—¶è‡ªåŠ¨é‡å¯
â”‚                                # - ç»Ÿä¸€çš„è¾“å‡ºç›®å½•ç®¡ç†
â”‚                                # ä½¿ç”¨:
â”‚                                # vila-run -J my_job -N 2 --gpus-per-node 8 \
â”‚                                #   -m train python train.py
â”‚                                # è¦æ±‚:
â”‚                                # - è®¾ç½® VILA_SLURM_ACCOUNT ç¯å¢ƒå˜é‡
â”‚                                # - è®¾ç½® VILA_SLURM_PARTITION ç¯å¢ƒå˜é‡
â”‚
â””â”€â”€ eval.py                     # æ¨¡å‹è¯„ä¼°æ‰¹å¤„ç†å·¥å…· â­è¯„ä¼°å¿…å¤‡
                                 # åŠŸèƒ½:
                                 # - åœ¨å¤šä¸ªbenchmarkä¸Šæ‰¹é‡è¯„ä¼°æ¨¡å‹
                                 # - æ”¯æŒä»»åŠ¡è¿‡æ»¤ï¼ˆæŒ‰åç§°ã€æ ‡ç­¾ï¼‰
                                 # - è‡ªåŠ¨è·³è¿‡å·²å®Œæˆçš„ä»»åŠ¡
                                 # - ç»“æœæ±‡æ€»å’Œå¯è§†åŒ–
                                 # ä½¿ç”¨:
                                 # vila-eval -m /path/to/model -c llama_3
                                 # vila-eval -m model -c llama_3 -t mme,pope
                                 # vila-eval -m model -c llama_3 -i image
```

### ğŸ“ data/ - æ•°æ®å¤„ç†æ¨¡å—

```
data/
â”œâ”€â”€ __init__.py                 # æ•°æ®æ¨¡å—åˆå§‹åŒ–
â”‚                                # è¯´æ˜æ•°æ®åŠ è½½å’Œé¢„å¤„ç†åŠŸèƒ½
â”‚
â”œâ”€â”€ base.py                     # æ•°æ®é›†åŸºç±»
â”‚                                # - BaseDataset: æ‰€æœ‰æ•°æ®é›†çš„åŸºç±»
â”‚                                # - å®šä¹‰æ ‡å‡†æ¥å£å’Œé€šç”¨æ–¹æ³•
â”‚
â”œâ”€â”€ builder.py                  # æ•°æ®é›†æ„å»ºå™¨ â­æ ¸å¿ƒ
â”‚                                # - make_data_module(): åˆ›å»ºæ•°æ®æ¨¡å—
â”‚                                # - æ ¹æ®é…ç½®è‡ªåŠ¨æ„å»ºDataLoader
â”‚                                # - æ”¯æŒæ•°æ®é›†æ··åˆ
â”‚
â”œâ”€â”€ dataset.py                  # æ ‡å‡†æ•°æ®é›†å®ç°
â”‚                                # - LazySupervisedDataset: å»¶è¿ŸåŠ è½½æ•°æ®é›†
â”‚                                # - å¤„ç†JSONæ ¼å¼çš„å¯¹è¯æ•°æ®
â”‚                                # - æ”¯æŒå›¾åƒå’Œè§†é¢‘
â”‚
â”œâ”€â”€ datasets_mixture.py         # å¤šæ•°æ®é›†æ··åˆ â­é‡è¦
â”‚                                # - DatasetMixture: æ··åˆå¤šä¸ªæ•°æ®é›†
â”‚                                # - æ”¯æŒé‡‡æ ·æ¯”ä¾‹æ§åˆ¶
â”‚                                # - åŠ¨æ€æ•°æ®å¢å¼º
â”‚
â”œâ”€â”€ simple_vila_webdataset.py  # WebDatasetæ”¯æŒ
â”‚                                # - VILAæ ¼å¼çš„WebDatasetåŠ è½½å™¨
â”‚                                # - é«˜æ•ˆçš„æµå¼æ•°æ®å¤„ç†
â”‚
â”œâ”€â”€ utils.py                    # æ•°æ®å·¥å…·å‡½æ•°
â”‚                                # - æ•°æ®é¢„å¤„ç†å‡½æ•°
â”‚                                # - æ•°æ®æ ¼å¼è½¬æ¢
â”‚
â”œâ”€â”€ dataset_impl/               # å…·ä½“æ•°æ®é›†å®ç°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ llava.py               # LLaVAæ ¼å¼æ•°æ®é›†
â”‚                                # - æ”¯æŒShareGPTæ ¼å¼
â”‚                                # - æ”¯æŒå¤šè½®å¯¹è¯
â”‚
â””â”€â”€ registry/                   # æ•°æ®é›†æ³¨å†Œè¡¨
    â””â”€â”€ default.yaml           # é»˜è®¤æ•°æ®é›†é…ç½®
                                 # - æ³¨å†Œæ‰€æœ‰å¯ç”¨æ•°æ®é›†
                                 # - é…ç½®æ•°æ®è·¯å¾„å’Œæ ¼å¼
```

### ğŸ“ eval/ - è¯„ä¼°æ¨¡å—

```
eval/
â”œâ”€â”€ __init__.py                 # è¯„ä¼°æ¨¡å—åˆå§‹åŒ–
â”‚                                # - EVAL_ROOT: è¯„ä¼°è„šæœ¬æ ¹ç›®å½•
â”‚                                # - TASKS: æ‰€æœ‰è¯„ä¼°ä»»åŠ¡çš„é…ç½®
â”‚
â”œâ”€â”€ run_navigation.py          # å¯¼èˆªä»»åŠ¡è¯„ä¼° â­å¯¼èˆªä¸“ç”¨
â”‚                                # - R2Rè¯„ä¼°
â”‚                                # - RxRè¯„ä¼°
â”‚                                # - è‡ªåŠ¨è®¡ç®—SPLã€SRç­‰æŒ‡æ ‡
â”‚
â”œâ”€â”€ run_vila.py                # VILAæ¨¡å‹é€šç”¨è¯„ä¼°
â”‚                                # - å›¾åƒç†è§£ä»»åŠ¡
â”‚                                # - è§†é¢‘ç†è§£ä»»åŠ¡
â”‚
â”œâ”€â”€ eval_textvqa.py            # TextVQAè¯„ä¼°
â”œâ”€â”€ eval_refcoco.py            # RefCOCOç›®æ ‡å®šä½è¯„ä¼°
â”œâ”€â”€ eval_mathvista.py          # MathVistaæ•°å­¦æ¨ç†è¯„ä¼°
â”œâ”€â”€ eval_mmmu.py               # MMMUå¤šæ¨¡æ€ç†è§£è¯„ä¼°
â”‚
â”œâ”€â”€ model_vqa_*.py             # å„ç§VQAä»»åŠ¡çš„æ¨¡å‹æ¥å£
â”‚   â”œâ”€â”€ model_vqa_loader.py   # é€šç”¨VQAåŠ è½½å™¨
â”‚   â”œâ”€â”€ model_vqa_video.py    # è§†é¢‘VQA
â”‚   â”œâ”€â”€ model_vqa_ego_schema.py  # EgoSchemaè¯„ä¼°
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ lmms/                      # LMMSè¯„ä¼°æ¡†æ¶é›†æˆ
â”‚   â”œâ”€â”€ models/               # LMMSæ¨¡å‹æ¥å£
â”‚   â””â”€â”€ tasks/                # LMMSä»»åŠ¡å®šä¹‰
â”‚
â”œâ”€â”€ mathvista_utils/          # MathVistaå·¥å…·
â”œâ”€â”€ mmmu_utils/               # MMMUå·¥å…·
â”œâ”€â”€ vision_niah_vila/         # Vision NIAHè¯„ä¼°
â”‚
â””â”€â”€ registry.yaml             # è¯„ä¼°ä»»åŠ¡æ³¨å†Œè¡¨
                               # - å®šä¹‰æ‰€æœ‰å¯è¯„ä¼°çš„ä»»åŠ¡
                               # - æŒ‡å®šè¯„ä¼°è„šæœ¬å’ŒæŒ‡æ ‡è·¯å¾„
```

### ğŸ“ model/ - æ¨¡å‹å®šä¹‰æ¨¡å—

```
model/
â”œâ”€â”€ __init__.py                 # æ¨¡å‹æ¨¡å—åˆå§‹åŒ– â­é‡è¦
â”‚                                # - å¯¼å‡ºæ‰€æœ‰æ¨¡å‹ç±»
â”‚                                # - æ³¨å†ŒHuggingFaceæ¨¡å‹
â”‚
â”œâ”€â”€ builder.py                  # æ¨¡å‹æ„å»ºå™¨ â­æ ¸å¿ƒ
â”‚                                # - load_pretrained_model(): åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
â”‚                                # - å¤„ç†æ¨¡å‹åˆå¹¶å’ŒLoRA
â”‚
â”œâ”€â”€ llava_arch.py              # æ ¸å¿ƒæ¶æ„å®šä¹‰ â­æœ€é‡è¦
â”‚                                # - LlavaMetaModel: æ¨¡å‹åŸºç±»
â”‚                                # - LlavaMetaForCausalLM: å› æœè¯­è¨€æ¨¡å‹åŸºç±»
â”‚                                # - å®šä¹‰encode_images()ç­‰æ ¸å¿ƒæ–¹æ³•
â”‚
â”œâ”€â”€ configuration_llava.py     # æ¨¡å‹é…ç½®ç±»
â”‚                                # - LlavaConfig: ç»§æ‰¿è‡ªPretrainedConfig
â”‚                                # - å®šä¹‰æ‰€æœ‰æ¨¡å‹è¶…å‚æ•°
â”‚
â”œâ”€â”€ loss.py                    # æŸå¤±å‡½æ•°
â”‚                                # - è‡ªå®šä¹‰æŸå¤±å‡½æ•°
â”‚                                # - å¤šä»»åŠ¡æŸå¤±ç»„åˆ
â”‚
â”œâ”€â”€ utils.py                   # æ¨¡å‹å·¥å…·å‡½æ•°
â”‚
â”œâ”€â”€ apply_delta.py             # åº”ç”¨æ¨¡å‹å¢é‡
â”œâ”€â”€ make_delta.py              # ç”Ÿæˆæ¨¡å‹å¢é‡
â”œâ”€â”€ consolidate.py             # æ¨¡å‹åˆå¹¶å·¥å…·
â”‚
â”œâ”€â”€ language_model/            # è¯­è¨€æ¨¡å‹åç«¯ â­å¯æ‰©å±•
â”‚   â”œâ”€â”€ builder.py            # è¯­è¨€æ¨¡å‹æ„å»ºå™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ llava_llama.py        # LLaMAåç«¯ (æ¨è)
â”‚   â”‚                          # - LlavaLlamaForCausalLM
â”‚   â”‚                          # - æ”¯æŒLLaMA 2/3
â”‚   â”‚
â”‚   â”œâ”€â”€ llava_mistral.py      # Mistralåç«¯
â”‚   â”‚                          # - LlavaMistralForCausalLM
â”‚   â”‚                          # - 7Bé«˜æ•ˆæ¨¡å‹
â”‚   â”‚
â”‚   â”œâ”€â”€ llava_mixtral.py      # Mixtralåç«¯ (MoE)
â”‚   â”‚                          # - LlavaMixtralForCausalLM
â”‚   â”‚                          # - ä¸“å®¶æ··åˆæ¶æ„
â”‚   â”‚
â”‚   â”œâ”€â”€ llava_gemma.py        # Gemmaåç«¯
â”‚   â”‚                          # - LlavaGemmaForCausalLM
â”‚   â”‚                          # - Googleå¼€æºæ¨¡å‹
â”‚   â”‚
â”‚   â”œâ”€â”€ llava_qwen.py         # Qwenåç«¯
â”‚   â”œâ”€â”€ llava_phi3.py         # Phi-3åç«¯
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ multimodal_encoder/       # è§†è§‰ç¼–ç å™¨ â­å¯é€‰æ‹©
â”‚   â”œâ”€â”€ builder.py           # è§†è§‰ç¼–ç å™¨æ„å»ºå™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ siglip_encoder.py    # SigLIPç¼–ç å™¨ (æ¨è)
â”‚   â”‚                         # - Googleçš„SigLIPæ¨¡å‹
â”‚   â”‚                         # - é«˜è´¨é‡è§†è§‰ç‰¹å¾
â”‚   â”‚
â”‚   â”œâ”€â”€ clip_encoder.py      # CLIPç¼–ç å™¨
â”‚   â”‚                         # - OpenAIçš„CLIPæ¨¡å‹
â”‚   â”‚                         # - ç»å…¸é€‰æ‹©
â”‚   â”‚
â”‚   â”œâ”€â”€ intern_encoder.py    # InternViTç¼–ç å™¨
â”‚   â”‚                         # - é«˜åˆ†è¾¨ç‡æ”¯æŒ
â”‚   â”‚                         # - 6Bå‚æ•°
â”‚   â”‚
â”‚   â”œâ”€â”€ radio_encoder.py     # RADIOç¼–ç å™¨
â”‚   â”‚                         # - é²æ£’æ€§è§†è§‰ç¼–ç 
â”‚   â”‚
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ multimodal_projector/     # å¤šæ¨¡æ€æŠ•å½±å™¨
    â”œâ”€â”€ builder.py           # æŠ•å½±å™¨æ„å»ºå™¨
    â”‚
    â””â”€â”€ base_projector.py    # åŸºç¡€æŠ•å½±å™¨å®ç°
                              # - MLPProjector: ç®€å•MLP
                              # - MLPDownsampleProjector: å¸¦ä¸‹é‡‡æ ·
                              # - å°†è§†è§‰tokenæ˜ å°„åˆ°è¯­è¨€ç©ºé—´
```

### ğŸ“ train/ - è®­ç»ƒæ¨¡å—

```
train/
â”œâ”€â”€ __init__.py                 # è®­ç»ƒæ¨¡å—åˆå§‹åŒ–
â”‚                                # è¯´æ˜è®­ç»ƒæµç¨‹å’Œä¼˜åŒ–ç­–ç•¥
â”‚
â”œâ”€â”€ train.py                    # åŸºç¡€è®­ç»ƒè„šæœ¬
â”‚                                # - æ ‡å‡†çš„Transformers Trainerè®­ç»ƒæµç¨‹
â”‚                                # - é€‚ç”¨äºç®€å•åœºæ™¯
â”‚
â”œâ”€â”€ train_mem.py                # å†…å­˜ä¼˜åŒ–è®­ç»ƒ â­æ¨è
â”‚                                # - å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
â”‚                                # - DeepSpeed ZeROé›†æˆ
â”‚                                # - é€‚åˆå¤§æ¨¡å‹å’Œé•¿åºåˆ—
â”‚
â”œâ”€â”€ train_long.py               # é•¿åºåˆ—è®­ç»ƒ
â”‚                                # - åºåˆ—å¹¶è¡Œæ”¯æŒ
â”‚                                # - å¤„ç†è¶…é•¿ä¸Šä¸‹æ–‡
â”‚
â”œâ”€â”€ train_hybrid.py             # æ··åˆå¹¶è¡Œè®­ç»ƒ
â”‚                                # - æ•°æ®å¹¶è¡Œ + æ¨¡å‹å¹¶è¡Œ + åºåˆ—å¹¶è¡Œ
â”‚                                # - æœ€å¤§åŒ–è®­ç»ƒæ•ˆç‡
â”‚
â”œâ”€â”€ llava_trainer.py           # è‡ªå®šä¹‰Trainer â­æ ¸å¿ƒ
â”‚                                # - LLaVATrainer: ç»§æ‰¿è‡ªTransformers Trainer
â”‚                                # - è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯
â”‚                                # - æ”¯æŒå¤šæ¨¡æ€æ•°æ®
â”‚
â”œâ”€â”€ args.py                    # è®­ç»ƒå‚æ•°å®šä¹‰
â”‚                                # - ModelArguments: æ¨¡å‹ç›¸å…³å‚æ•°
â”‚                                # - DataArguments: æ•°æ®ç›¸å…³å‚æ•°
â”‚                                # - TrainingArguments: è®­ç»ƒç›¸å…³å‚æ•°
â”‚
â”œâ”€â”€ utils.py                   # è®­ç»ƒå·¥å…·å‡½æ•°
â”‚                                # - å­¦ä¹ ç‡è°ƒåº¦
â”‚                                # - Checkpointç®¡ç†
â”‚
â”œâ”€â”€ callbacks/                 # è®­ç»ƒå›è°ƒ
â”‚   â””â”€â”€ autoresume_callback.py # è‡ªåŠ¨æ¢å¤å›è°ƒ
â”‚                                # - æ”¯æŒè®­ç»ƒä¸­æ–­åè‡ªåŠ¨æ¢å¤
â”‚
â”œâ”€â”€ sequence_parallel/         # åºåˆ—å¹¶è¡Œå®ç° â­é«˜çº§
â”‚   â”œâ”€â”€ ulysses_attn.py       # Ulyssesæ³¨æ„åŠ›
â”‚   â”‚                          # - è·¨GPUçš„æ³¨æ„åŠ›å¹¶è¡Œ
â”‚   â”‚
â”‚   â”œâ”€â”€ hybrid_attn.py        # æ··åˆæ³¨æ„åŠ›
â”‚   â”‚                          # - Ulysses + Ringçš„æ··åˆç­–ç•¥
â”‚   â”‚
â”‚   â””â”€â”€ ring/                 # Ringæ³¨æ„åŠ›
â”‚       â””â”€â”€ ...                # - ç¯å½¢é€šä¿¡çš„æ³¨æ„åŠ›å¹¶è¡Œ
â”‚
â”œâ”€â”€ deepspeed_replace/         # DeepSpeedè‡ªå®šä¹‰æ¨¡å—
â”‚   â””â”€â”€ ...                    # - æ›¿æ¢DeepSpeedçš„é»˜è®¤å®ç°
â”‚                               # - ä¼˜åŒ–é€šä¿¡å’Œå†…å­˜
â”‚
â””â”€â”€ transformers_replace/      # Transformersè‡ªå®šä¹‰æ¨¡å—
    â””â”€â”€ ...                    # - æ›¿æ¢Transformersçš„é»˜è®¤å®ç°
                                # - ä¼˜åŒ–æ¨ç†å’Œè®­ç»ƒ
```

### ğŸ“ utils/ - é€šç”¨å·¥å…·

```
utils/
â”œâ”€â”€ __init__.py                 # å·¥å…·æ¨¡å—åˆå§‹åŒ–
â”œâ”€â”€ distributed.py              # åˆ†å¸ƒå¼å·¥å…·
â”‚                                # - åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
â”‚                                # - è¿›ç¨‹é—´é€šä¿¡
â”‚
â”œâ”€â”€ io.py                       # IOå·¥å…·
â”‚                                # - JSON/YAMLåŠ è½½å’Œä¿å­˜
â”‚                                # - å®‰å…¨çš„æ–‡ä»¶æ“ä½œ
â”‚
â”œâ”€â”€ logging.py                  # æ—¥å¿—å·¥å…·
â”‚                                # - ç»Ÿä¸€çš„æ—¥å¿—æ ¼å¼
â”‚                                # - å¤šè¿›ç¨‹æ—¥å¿—ç®¡ç†
â”‚
â”œâ”€â”€ media.py                    # åª’ä½“å¤„ç†å·¥å…·
â”‚                                # - å›¾åƒ/è§†é¢‘æ ¼å¼è½¬æ¢
â”‚
â”œâ”€â”€ tokenizer.py                # Tokenizerå·¥å…·
â”‚                                # - ç‰¹æ®Štokenå¤„ç†
â”‚
â””â”€â”€ merge_lora_weights_and_save_hf_model.py  # LoRAåˆå¹¶å·¥å…·
                                              # - å°†LoRAæƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹
```

### ğŸ“ å…¶ä»–æ¨¡å—

```
trl/                            # å¼ºåŒ–å­¦ä¹ æ¨¡å— (TRLé›†æˆ)
â”œâ”€â”€ trainer/                    # RLHF Trainer
â””â”€â”€ models/                     # ä»·å€¼æ¨¡å‹å’Œå¥–åŠ±æ¨¡å‹

wids/                           # WebDatasetç´¢å¼•ç³»ç»Ÿ
â”œâ”€â”€ wids.py                    # æ ¸å¿ƒç´¢å¼•ç±»
â”œâ”€â”€ wids_dl.py                 # ä¸‹è½½å·¥å…·
â””â”€â”€ ...                        # ç´¢å¼•ç®¡ç†å’Œä¼˜åŒ–

data_aug/                       # æ•°æ®å¢å¼ºå·¥å…·
â”œâ”€â”€ caption2qa.py              # æ ‡é¢˜è½¬é—®ç­”
â””â”€â”€ video_inference.py         # è§†é¢‘æ¨ç†
```


## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…ä¾èµ–
cd NaVILA-NOTED
pip install -e .

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¦‚ä½¿ç”¨SLURMï¼‰
export VILA_SLURM_ACCOUNT=your_account
export VILA_SLURM_PARTITION=your_partition
```

### 2. åŠ è½½æ¨¡å‹

```python
import llava

# æ–¹æ³•1: ä½¿ç”¨ç®€åŒ–çš„loadæ¥å£ï¼ˆæ¨èï¼‰
model = llava.load("a8cheng/navila-siglip-llama3-8b-v1.5-pretrain")

# æ–¹æ³•2: æŒ‡å®šè®¾å¤‡
model = llava.load(
    "path/to/model",
    devices=[0, 1],  # ä½¿ç”¨GPU 0å’Œ1
    load_8bit=True   # 8-bité‡åŒ–
)

# æ–¹æ³•3: æ›´åº•å±‚çš„åŠ è½½æ–¹å¼
from llava.model.builder import load_pretrained_model

tokenizer, model, image_processor, context_len = load_pretrained_model(
    model_path="a8cheng/navila-siglip-llama3-8b-v1.5-pretrain",
    model_base=None,
    model_name="navila"
)
```

### 3. å›¾åƒç†è§£

```python
from llava.media import Image
from llava.conversation import conv_llama_3
from llava.mm_utils import process_images, tokenizer_image_token
from llava.constants import IMAGE_TOKEN_INDEX

# åŠ è½½å›¾åƒ
image = Image("/path/to/image.jpg")

# å‡†å¤‡å¯¹è¯
conv = conv_llama_3.copy()
conv.append_message(conv.roles[0], "<image>\nè¯·æè¿°è¿™å¼ å›¾ç‰‡ä¸­çš„å†…å®¹")
conv.append_message(conv.roles[1], None)
prompt = conv.get_prompt()

# å¤„ç†å›¾åƒ
image_tensor = process_images([image.data], image_processor, model.config)

# TokenåŒ–
input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX)

# æ¨ç†
with torch.inference_mode():
    output_ids = model.generate(
        input_ids.unsqueeze(0).cuda(),
        images=image_tensor.cuda(),
        max_new_tokens=512,
        do_sample=True,
        temperature=0.7
    )

# è§£ç è¾“å‡º
response = tokenizer.decode(output_ids[0], skip_special_tokens=True)
print(response)
```

### 4. è§†é¢‘ç†è§£

```python
from llava.media import Video
from llava.mm_utils import opencv_extract_frames, process_images

# ä»è§†é¢‘æå–å¸§
frames = opencv_extract_frames(
    video_path="/path/to/video.mp4",
    frames=8,   # æå–8å¸§
    fps=3       # æœ€å¤§FPSé™åˆ¶
)

# å¤„ç†è§†é¢‘å¸§ï¼ˆä¸å›¾åƒå¤„ç†ç›¸åŒï¼‰
video_tensor = process_images(frames, image_processor, model.config)

# æ¨ç†ï¼ˆä½¿ç”¨8ä¸ª<image> tokenï¼‰
prompt = "<image> " * 8 + "\næè¿°è¿™ä¸ªè§†é¢‘ä¸­å‘ç”Ÿäº†ä»€ä¹ˆ"
# ... åç»­å¤„ç†ä¸å›¾åƒç›¸åŒ
```

### 5. è®­ç»ƒæ¨¡å‹

**å•æœºå¤šå¡è®­ç»ƒï¼š**
```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export n_node=1
export GPUS_PER_NODE=4
export MASTER_PORT=29500
export MASTER_ADDR=localhost
export CURRENT_RANK=0

# è¿è¡Œè®­ç»ƒè„šæœ¬
bash scripts/train/sft_8frames.sh
```

**SLURMé›†ç¾¤è®­ç»ƒï¼š**
```bash
# ä½¿ç”¨vila-runå·¥å…·
vila-run \
  -J navila_training \
  -N 2 \
  --gpus-per-node 8 \
  -m train \
  -t 48:00:00 \
  bash scripts/train/sft_8frames.sh
```

**è®­ç»ƒå‚æ•°è¯´æ˜ï¼š**
```bash
# åœ¨ sft_8frames.sh ä¸­ä¿®æ”¹å…³é”®å‚æ•°ï¼š
--model_name_or_path      # åŸºç¡€æ¨¡å‹è·¯å¾„
--vision_tower            # è§†è§‰ç¼–ç å™¨
--data_mixture            # æ•°æ®é›†æ··åˆ
--num_video_frames 8      # è§†é¢‘å¸§æ•°
--per_device_train_batch_size 10  # æ‰¹æ¬¡å¤§å°
--learning_rate 1e-4      # å­¦ä¹ ç‡
--num_train_epochs 1      # è®­ç»ƒè½®æ•°
```

### 6. è¯„ä¼°æ¨¡å‹

**è¯„ä¼°æ‰€æœ‰ä»»åŠ¡ï¼š**
```bash
vila-eval \
  -m /path/to/checkpoint \
  -c llama_3 \
  -n 8
```

**è¯„ä¼°ç‰¹å®šä»»åŠ¡ï¼š**
```bash
# åªè¯„ä¼°MMEå’ŒPOPE
vila-eval \
  -m /path/to/checkpoint \
  -c llama_3 \
  -t mme,pope

# åªè¯„ä¼°å›¾åƒä»»åŠ¡ï¼ˆæ’é™¤è§†é¢‘ï¼‰
vila-eval \
  -m /path/to/checkpoint \
  -c llama_3 \
  -i image \
  -e video
```

## ğŸ”§ é…ç½®è¯´æ˜

### æ¨¡å‹é…ç½®

**å…³é”®å‚æ•°ï¼š**
| å‚æ•° | è¯´æ˜ | ç¤ºä¾‹å€¼ |
|------|------|--------|
| `--model_name_or_path` | åŸºç¡€è¯­è¨€æ¨¡å‹è·¯å¾„ | `meta-llama/Meta-Llama-3-8B-Instruct` |
| `--vision_tower` | è§†è§‰ç¼–ç å™¨ | `google/siglip-so400m-patch14-384` |
| `--mm_projector_type` | æŠ•å½±å™¨ç±»å‹ | `mlp_downsample` |
| `--num_video_frames` | è§†é¢‘å¸§æ•° | `8` |
| `--mm_vision_select_layer` | é€‰æ‹©è§†è§‰å±‚ | `-2` (å€’æ•°ç¬¬äºŒå±‚) |

**ç»„åˆæ¨èï¼š**
```bash
# é…ç½®1: SigLIP + LLaMA-3-8B (æ¨è)
--model_name_or_path meta-llama/Meta-Llama-3-8B-Instruct \
--vision_tower google/siglip-so400m-patch14-384 \
--mm_projector_type mlp_downsample

# é…ç½®2: CLIP + Mistral-7B (ç»æµ)
--model_name_or_path mistralai/Mistral-7B-Instruct-v0.2 \
--vision_tower openai/clip-vit-large-patch14-336 \
--mm_projector_type mlp

# é…ç½®3: InternViT + LLaMA-3-70B (é«˜æ€§èƒ½)
--model_name_or_path meta-llama/Meta-Llama-3-70B-Instruct \
--vision_tower OpenGVLab/InternViT-6B-448px-V1-5 \
--mm_projector_type mlp_downsample
```

### æ•°æ®é…ç½®

**æ•°æ®é›†æ··åˆç­–ç•¥ï¼š**
```bash
# å¯¼èˆªä»»åŠ¡ä¸“ç”¨
--data_mixture r2r+rxr+envdrop+human+scanqa

# é€šç”¨è§†è§‰ç†è§£
--data_mixture sharegpt4v+video_chatgpt+llava_instruct

# æ··åˆè®­ç»ƒï¼ˆå¯¼èˆª + é€šç”¨ï¼‰
--data_mixture r2r+rxr+sharegpt4v+video_chatgpt
```

**æ•°æ®è·¯å¾„é…ç½®ï¼š**
```bash
--data_path /path/to/NaVILA-Dataset  # æ•°æ®é›†æ ¹ç›®å½•
--image_folder /path/to/images       # å›¾åƒæ–‡ä»¶å¤¹
--video_folder /path/to/videos       # è§†é¢‘æ–‡ä»¶å¤¹
```

### è®­ç»ƒé…ç½®

**æ‰¹æ¬¡å¤§å°è®¡ç®—ï¼š**
```
å…¨å±€æ‰¹æ¬¡å¤§å° = per_device_batch_size Ã— num_gpus Ã— gradient_accumulation_steps
```

**é…ç½®ç¤ºä¾‹ï¼š**
| åœºæ™¯ | per_device_batch | accumulation_steps | å…¨å±€æ‰¹æ¬¡ | æ˜¾å­˜éœ€æ±‚ |
|------|------------------|--------------------|----------|----------|
| å°è§„æ¨¡å®éªŒ | 4 | 1 | 32 (8 GPU) | ~24GB |
| æ ‡å‡†è®­ç»ƒ | 10 | 2 | 160 (8 GPU) | ~40GB |
| å¤§è§„æ¨¡è®­ç»ƒ | 16 | 4 | 512 (8 GPU) | ~80GB |

**å­¦ä¹ ç‡ç­–ç•¥ï¼š**
```bash
--learning_rate 1e-4              # åŸºç¡€å­¦ä¹ ç‡
--lr_scheduler_type cosine        # ä½™å¼¦é€€ç«
--warmup_ratio 0.03               # é¢„çƒ­æ¯”ä¾‹
--weight_decay 0.0                # æƒé‡è¡°å‡
```

**ä¿å­˜ç­–ç•¥ï¼š**
```bash
--save_strategy epoch             # æ¯ä¸ªepochä¿å­˜
--save_total_limit 2              # æœ€å¤šä¿ç•™2ä¸ªcheckpoint
--save_steps 1000                 # æˆ–æ¯1000æ­¥ä¿å­˜
```

## ğŸ“Š æ”¯æŒçš„ä»»åŠ¡

### 1. è§†è§‰-è¯­è¨€å¯¼èˆª (VLN) â­æ ¸å¿ƒä»»åŠ¡

| ä»»åŠ¡ | æ•°æ®é›† | æŒ‡æ ‡ | è¯´æ˜ |
|------|--------|------|------|
| **å®¤å†…å¯¼èˆª** | R2R | SR, SPL, nDTW | æ ¹æ®æŒ‡ä»¤åœ¨å®¤å†…åœºæ™¯ä¸­å¯¼èˆª |
| **å¤šè¯­è¨€å¯¼èˆª** | RxR | SR, SPL | æ”¯æŒè‹±è¯­ã€å°åœ°è¯­ã€æ³°å¢å›ºè¯­ |
| **ç¯å¢ƒæ³›åŒ–** | EnvDrop | SR, SPL | æµ‹è¯•æ¨¡å‹åœ¨æ–°ç¯å¢ƒçš„æ³›åŒ–èƒ½åŠ› |
| **3Dåœºæ™¯é—®ç­”** | ScanQA | CIDEr, BLEU | 3Dåœºæ™¯ä¸­çš„é—®ç­”ä»»åŠ¡ |

### 2. è§†é¢‘ç†è§£

| ä»»åŠ¡ | Benchmark | è¯´æ˜ |
|------|-----------|------|
| **è§†é¢‘é—®ç­”** | Video-ChatGPT | å¼€æ”¾å¼è§†é¢‘é—®ç­” |
| **è§†é¢‘æè¿°** | MSVD, MSR-VTT | è§†é¢‘æ ‡é¢˜ç”Ÿæˆ |
| **æ—¶é—´æ¨ç†** | NExT-QA | æ—¶åºå…³ç³»ç†è§£ |
| **é•¿è§†é¢‘ç†è§£** | EgoSchema | é•¿è§†é¢‘åœºæ™¯ç†è§£ |

### 3. å›¾åƒç†è§£

| ä»»åŠ¡ | Benchmark | è¯´æ˜ |
|------|-----------|------|
| **é€šç”¨VQA** | VQAv2, GQA | å›¾åƒé—®ç­” |
| **OCR** | TextVQA, DocVQA | æ–‡æœ¬è¯†åˆ«å’Œç†è§£ |
| **ç»†ç²’åº¦è¯†åˆ«** | RefCOCO | ç›®æ ‡å®šä½å’Œå¼•ç”¨ |
| **æ¨ç†** | POPE, MME | å¹»è§‰æ£€æµ‹å’Œå¤šæ¨¡æ€è¯„ä¼° |
| **æ•°å­¦æ¨ç†** | MathVista | è§†è§‰æ•°å­¦é—®é¢˜ |
| **ç§‘å­¦æ¨ç†** | MMMU | å¤šå­¦ç§‘ç†è§£ |

### 4. ç‰¹æ®Šèƒ½åŠ›

- **å¤šè¯­è¨€æ”¯æŒ**: è‹±è¯­ã€ä¸­æ–‡ã€å°åœ°è¯­ã€æ³°å¢å›ºè¯­
- **ç©ºé—´æ¨ç†**: 3Dåœºæ™¯ç†è§£å’Œç©ºé—´å…³ç³»
- **æ—¶åºæ¨ç†**: è§†é¢‘ä¸­çš„æ—¶é—´å…³ç³»ç†è§£
- **é•¿ä¸Šä¸‹æ–‡**: æ”¯æŒè¶…é•¿åºåˆ—ï¼ˆ>32K tokensï¼‰

## ğŸ› ï¸ å¼€å‘æŒ‡å—

### 1. æ·»åŠ æ–°çš„æ•°æ®é›†

**æ­¥éª¤ï¼š**
```bash
# 1. åˆ›å»ºæ•°æ®é›†å®ç°æ–‡ä»¶
touch llava/data/dataset_impl/my_dataset.py
```

```python
# 2. å®ç°æ•°æ®é›†ç±»
from llava.data.base import BaseDataset

class MyDataset(BaseDataset):
    def __init__(self, data_path, **kwargs):
        super().__init__()
        # åŠ è½½æ•°æ®
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        # è¿”å›å•ä¸ªæ ·æœ¬
        return {
            "image": image_path,
            "conversations": conversations
        }
```

```yaml
# 3. åœ¨ data/registry/default.yaml æ³¨å†Œ
my_dataset:
  type: llava.data.dataset_impl.my_dataset.MyDataset
  data_path: /path/to/data
  description: "æˆ‘çš„è‡ªå®šä¹‰æ•°æ®é›†"
```

```bash
# 4. ä½¿ç”¨æ•°æ®é›†
--data_mixture my_dataset+r2r
```

### 2. æ·»åŠ æ–°çš„æ¨¡å‹åç«¯

**æ­¥éª¤ï¼š**
```bash
# 1. åˆ›å»ºæ¨¡å‹æ–‡ä»¶
touch llava/model/language_model/llava_mymodel.py
```

```python
# 2. å®ç°æ¨¡å‹ç±»
from llava.model.llava_arch import LlavaMetaForCausalLM
from transformers import AutoModelForCausalLM, AutoConfig

class LlavaMyModelConfig(LlavaConfig):
    model_type = "llava_mymodel"

class LlavaMyModelForCausalLM(MyModelForCausalLM, LlavaMetaForCausalLM):
    config_class = LlavaMyModelConfig
    
    def __init__(self, config):
        super().__init__(config)
```

```python
# 3. åœ¨ model/__init__.py æ³¨å†Œ
from .language_model.llava_mymodel import (
    LlavaMyModelConfig,
    LlavaMyModelForCausalLM
)

AutoConfig.register("llava_mymodel", LlavaMyModelConfig)
AutoModelForCausalLM.register(LlavaMyModelConfig, LlavaMyModelForCausalLM)
```

### 3. æ·»åŠ æ–°çš„è§†è§‰ç¼–ç å™¨

**æ­¥éª¤ï¼š**
```bash
# 1. åˆ›å»ºç¼–ç å™¨æ–‡ä»¶
touch llava/model/multimodal_encoder/my_encoder.py
```

```python
# 2. å®ç°ç¼–ç å™¨ç±»
import torch.nn as nn

class MyVisionTower(nn.Module):
    def __init__(self, vision_tower, args, delay_load=False):
        super().__init__()
        self.vision_tower_name = vision_tower
        # åˆå§‹åŒ–è§†è§‰ç¼–ç å™¨
        
    def forward(self, images):
        # ç¼–ç å›¾åƒ
        return image_features
```

```python
# 3. åœ¨ multimodal_encoder/builder.py æ³¨å†Œ
def build_vision_tower(vision_tower_cfg, **kwargs):
    vision_tower = getattr(vision_tower_cfg, 'mm_vision_tower', getattr(vision_tower_cfg, 'vision_tower', None))
    
    if 'my_encoder' in vision_tower.lower():
        return MyVisionTower(vision_tower, args=vision_tower_cfg, **kwargs)
    # ...
```

### 4. è°ƒè¯•æŠ€å·§

**æ‰“å°æ¨¡å‹æ¶æ„ï¼š**
```python
print(model)
print(f"Total parameters: {sum(p.numel() for p in model.parameters())}")
```

**ç›‘æ§æ˜¾å­˜ä½¿ç”¨ï¼š**
```bash
watch -n 1 nvidia-smi
```

**å¯ç”¨è¯¦ç»†æ—¥å¿—ï¼š**
```bash
export LOGLEVEL=DEBUG
python your_script.py
```

**ä½¿ç”¨å°æ•°æ®é›†æµ‹è¯•ï¼š**
```bash
# å¿«é€ŸéªŒè¯æµç¨‹
--max_steps 10 \
--save_steps 5 \
--eval_steps 5
```

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **å†…å­˜ç®¡ç†**: 
   - è§†é¢‘ä»»åŠ¡éœ€è¦å¤§é‡æ˜¾å­˜
   - ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹å’ŒDeepSpeed ZeRO
   - å»ºè®®ä½¿ç”¨`train_mem.py`

2. **æ•°æ®é¢„å¤„ç†**:
   - è§†é¢‘å¸§æå–å¯èƒ½å¾ˆæ…¢
   - å»ºè®®é¢„å…ˆæå–å¸§åˆ°ç£ç›˜
   - ä½¿ç”¨`lazy_preprocess`å»¶è¿ŸåŠ è½½

3. **åˆ†å¸ƒå¼è®­ç»ƒ**:
   - ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹ç½‘ç»œäº’é€š
   - æ­£ç¡®è®¾ç½®ç¯å¢ƒå˜é‡
   - ä½¿ç”¨ç›¸åŒçš„é…ç½®æ–‡ä»¶

4. **æ¨¡å‹å…¼å®¹æ€§**:
   - ä¸åŒè¯­è¨€æ¨¡å‹éœ€è¦ä¸åŒçš„å¯¹è¯æ¨¡æ¿
   - æ³¨æ„tokenizerçš„ç‰¹æ®Štoken
   - æ£€æŸ¥æ¨¡å‹é…ç½®æ–‡ä»¶

## ğŸ”— ç›¸å…³èµ„æº

- **LLaVAåŸå§‹é¡¹ç›®**: https://github.com/haotian-liu/LLaVA/
- **VILAé¡¹ç›®**: https://github.com/Efficient-Large-Model/VILA

